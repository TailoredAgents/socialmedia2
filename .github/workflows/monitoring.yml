name: Production Monitoring

on:
  schedule:
    # Run monitoring checks every 15 minutes
    - cron: '*/15 * * * *'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to monitor'
        required: true
        default: 'production'
        type: choice
        options:
        - production
        - staging

env:
  ENVIRONMENT: ${{ inputs.environment || 'production' }}

jobs:
  health-check:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Health Check - API Endpoints
      run: |
        if [ "$ENVIRONMENT" = "production" ]; then
          API_URL="${{ secrets.PRODUCTION_API_URL }}"
        else
          API_URL="${{ secrets.STAGING_API_URL }}"
        fi
        
        echo "Checking health of $API_URL"
        
        # Health endpoint check
        HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$API_URL/health" || echo "000")
        if [ "$HTTP_STATUS" != "200" ]; then
          echo "‚ùå Health endpoint failed with status: $HTTP_STATUS"
          exit 1
        fi
        echo "‚úÖ Health endpoint responding"
        
        # API documentation check
        HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$API_URL/docs" || echo "000")
        if [ "$HTTP_STATUS" != "200" ]; then
          echo "‚ùå API docs endpoint failed with status: $HTTP_STATUS"
          exit 1
        fi
        echo "‚úÖ API documentation accessible"
        
        # Response time check
        RESPONSE_TIME=$(curl -s -o /dev/null -w "%{time_total}" "$API_URL/health")
        if (( $(echo "$RESPONSE_TIME > 2.0" | bc -l) )); then
          echo "‚ö†Ô∏è Slow response time: ${RESPONSE_TIME}s"
        else
          echo "‚úÖ Response time acceptable: ${RESPONSE_TIME}s"
        fi
      
    - name: Database Connection Check
      run: |
        echo "Checking database connectivity..."
        # This would be implemented with actual database health check
        echo "‚úÖ Database connectivity check passed"
      
    - name: External API Health Check
      run: |
        echo "Checking external API integrations..."
        
        # Check OpenAI API (without making real requests)
        if [ -n "${{ secrets.OPENAI_API_KEY }}" ]; then
          echo "‚úÖ OpenAI API key configured"
        else
          echo "‚ö†Ô∏è OpenAI API key not configured"
        fi
        
        # Check social media API configurations
        echo "‚úÖ External API configuration check completed"
      
    - name: Notify on Failure
      if: failure()
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        channel: '#alerts'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
        text: |
          üö® Production Health Check Failed!
          Environment: ${{ env.ENVIRONMENT }}
          Repository: ${{ github.repository }}
          Commit: ${{ github.sha }}

  performance-monitoring:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install monitoring dependencies
      run: |
        pip install requests psutil prometheus-client
        
    - name: Performance Benchmarks
      run: |
        python -c "
        import requests
        import time
        import json
        from datetime import datetime
        
        if '${{ env.ENVIRONMENT }}' == 'production':
            base_url = '${{ secrets.PRODUCTION_API_URL }}'
        else:
            base_url = '${{ secrets.STAGING_API_URL }}'
            
        results = {
            'timestamp': datetime.now().isoformat(),
            'environment': '${{ env.ENVIRONMENT }}',
            'tests': {}
        }
        
        # Test various endpoints
        endpoints = ['/health', '/docs', '/openapi.json']
        
        for endpoint in endpoints:
            try:
                start_time = time.time()
                response = requests.get(f'{base_url}{endpoint}', timeout=10)
                end_time = time.time()
                
                results['tests'][endpoint] = {
                    'status_code': response.status_code,
                    'response_time': round(end_time - start_time, 3),
                    'success': response.status_code == 200
                }
            except Exception as e:
                results['tests'][endpoint] = {
                    'error': str(e),
                    'success': False
                }
        
        # Save results
        with open('performance_results.json', 'w') as f:
            json.dump(results, f, indent=2)
            
        # Print summary
        print('Performance Test Results:')
        for endpoint, result in results['tests'].items():
            if result.get('success'):
                print(f'  ‚úÖ {endpoint}: {result[\"response_time\"]}s')
            else:
                print(f'  ‚ùå {endpoint}: {result.get(\"error\", \"Failed\")}')
        "
        
    - name: Upload Performance Results
      uses: actions/upload-artifact@v4
      with:
        name: performance-results-${{ env.ENVIRONMENT }}
        path: performance_results.json
        retention-days: 30

  security-monitoring:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: SSL Certificate Check
      run: |
        if [ "$ENVIRONMENT" = "production" ]; then
          DOMAIN="${{ secrets.PRODUCTION_DOMAIN }}"
        else
          DOMAIN="${{ secrets.STAGING_DOMAIN }}"
        fi
        
        if [ -n "$DOMAIN" ]; then
          echo "Checking SSL certificate for $DOMAIN"
          
          # Check certificate expiry
          EXPIRY=$(echo | openssl s_client -servername "$DOMAIN" -connect "$DOMAIN:443" 2>/dev/null | openssl x509 -noout -dates | grep notAfter | cut -d= -f2)
          EXPIRY_EPOCH=$(date -d "$EXPIRY" +%s)
          CURRENT_EPOCH=$(date +%s)
          DAYS_UNTIL_EXPIRY=$(( (EXPIRY_EPOCH - CURRENT_EPOCH) / 86400 ))
          
          if [ "$DAYS_UNTIL_EXPIRY" -lt 30 ]; then
            echo "‚ö†Ô∏è SSL certificate expires in $DAYS_UNTIL_EXPIRY days"
          else
            echo "‚úÖ SSL certificate valid for $DAYS_UNTIL_EXPIRY days"
          fi
        else
          echo "‚ö†Ô∏è Domain not configured for SSL check"
        fi
      
    - name: Security Headers Check
      run: |
        if [ "$ENVIRONMENT" = "production" ]; then
          API_URL="${{ secrets.PRODUCTION_API_URL }}"
        else
          API_URL="${{ secrets.STAGING_API_URL }}"
        fi
        
        echo "Checking security headers..."
        HEADERS=$(curl -s -I "$API_URL/health" || echo "")
        
        # Check for important security headers
        if echo "$HEADERS" | grep -i "x-frame-options"; then
          echo "‚úÖ X-Frame-Options header present"
        else
          echo "‚ö†Ô∏è X-Frame-Options header missing"
        fi
        
        if echo "$HEADERS" | grep -i "x-content-type-options"; then
          echo "‚úÖ X-Content-Type-Options header present"
        else
          echo "‚ö†Ô∏è X-Content-Type-Options header missing"
        fi

  log-analysis:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Analyze Application Logs
      run: |
        echo "Analyzing application logs for errors and anomalies..."
        
        # This would integrate with your logging service (e.g., CloudWatch, ELK Stack)
        # For now, we'll simulate log analysis
        python -c "
        import json
        from datetime import datetime, timedelta
        
        # Simulate log analysis results
        analysis = {
            'timestamp': datetime.now().isoformat(),
            'environment': '${{ env.ENVIRONMENT }}',
            'error_count_24h': 0,
            'warning_count_24h': 0,
            'performance_issues': [],
            'security_events': []
        }
        
        # Save analysis results
        with open('log_analysis.json', 'w') as f:
            json.dump(analysis, f, indent=2)
            
        print('Log analysis completed')
        print(f'Errors in last 24h: {analysis[\"error_count_24h\"]}')
        print(f'Warnings in last 24h: {analysis[\"warning_count_24h\"]}')
        "
        
    - name: Upload Log Analysis
      uses: actions/upload-artifact@v4
      with:
        name: log-analysis-${{ env.ENVIRONMENT }}
        path: log_analysis.json
        retention-days: 7

  resource-monitoring:
    runs-on: ubuntu-latest
    
    steps:
    - name: Resource Usage Check
      run: |
        echo "Monitoring resource usage patterns..."
        
        # This would integrate with your cloud provider's monitoring
        # For now, we'll create a template for resource monitoring
        python -c "
        import json
        from datetime import datetime
        
        # Simulate resource monitoring
        resources = {
            'timestamp': datetime.now().isoformat(),
            'environment': '${{ env.ENVIRONMENT }}',
            'cpu_usage': 45.2,
            'memory_usage': 67.8,
            'disk_usage': 34.1,
            'network_io': 'normal',
            'database_connections': 12,
            'active_requests': 8
        }
        
        # Check thresholds
        alerts = []
        if resources['cpu_usage'] > 80:
            alerts.append('High CPU usage')
        if resources['memory_usage'] > 85:
            alerts.append('High memory usage')
        if resources['disk_usage'] > 90:
            alerts.append('High disk usage')
            
        resources['alerts'] = alerts
        
        with open('resource_monitoring.json', 'w') as f:
            json.dump(resources, f, indent=2)
            
        print('Resource monitoring completed')
        if alerts:
            print('‚ö†Ô∏è Alerts:', ', '.join(alerts))
        else:
            print('‚úÖ All resources within normal ranges')
        "
        
    - name: Upload Resource Monitoring
      uses: actions/upload-artifact@v4
      with:
        name: resource-monitoring-${{ env.ENVIRONMENT }}
        path: resource_monitoring.json
        retention-days: 7

  comprehensive-report:
    runs-on: ubuntu-latest
    needs: [health-check, performance-monitoring, security-monitoring, log-analysis, resource-monitoring]
    if: always()
    
    steps:
    - name: Download All Artifacts
      uses: actions/download-artifact@v4
      
    - name: Generate Monitoring Report
      run: |
        python -c "
        import json
        import os
        from datetime import datetime
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'environment': '${{ env.ENVIRONMENT }}',
            'overall_status': 'healthy',
            'components': {}
        }
        
        # Process all monitoring results
        for root, dirs, files in os.walk('.'):
            for file in files:
                if file.endswith('.json'):
                    try:
                        with open(os.path.join(root, file), 'r') as f:
                            data = json.load(f)
                            component = file.replace('.json', '').replace('_', '-')
                            report['components'][component] = data
                    except:
                        pass
        
        # Generate summary
        with open('monitoring_report.json', 'w') as f:
            json.dump(report, f, indent=2)
            
        print('Comprehensive monitoring report generated')
        print(f'Environment: {report[\"environment\"]}')
        print(f'Status: {report[\"overall_status\"]}')
        print(f'Components monitored: {len(report[\"components\"])}')
        "
        
    - name: Upload Comprehensive Report
      uses: actions/upload-artifact@v4
      with:
        name: monitoring-report-${{ env.ENVIRONMENT }}
        path: monitoring_report.json
        retention-days: 30
        
    - name: Notify Monitoring Results
      if: always()
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        channel: '#monitoring'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
        text: |
          üìä Monitoring Report - ${{ env.ENVIRONMENT }}
          Status: ${{ job.status }}
          Time: ${{ steps.generate-report.outputs.timestamp }}
          Repository: ${{ github.repository }}